{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YoaV-F722tlR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import expm\n",
        "\n",
        "# fix seed\n",
        "seed=0 \n",
        "np.random.seed(seed)\n",
        "\n",
        "# fix output array\n",
        "np.set_printoptions(suppress=True,precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP5VvuX72tlO"
      },
      "source": [
        "# Deep Policy Gradient (PG)\n",
        "\n",
        "\n",
        "In this notebook, our goal is to implement the REINFORCE algorithm for policy gradient using [JAX](https://jax.readthedocs.io/en/latest/). We will apply this RL algorithm to control a single quantum bit of information (qubit).  \n",
        "\n",
        "## The REINFOCE Algorithm\n",
        "\n",
        "The reinforcement learning objective $J$ is the expected total return, following the policy $\\pi$. If the transition probability is denoted by $p(s'|s,a)$, and the initial state distribution is $p(s_0)$, the probability for a trajectory $\\tau = (s_0,a_0,r_1,s_1,a_1,\\dots,s_{T-1},a_{T-1},r_T,s_T)$ to occur can be written as\n",
        "\n",
        "$$ \n",
        "P_\\pi(\\tau) = p(s_0)\\prod_{t=1}^T \\pi(a_t|s_t)p(s_{t+1}|s_t,a_t). \n",
        "$$\n",
        "\n",
        "The RL objective then takes the form\n",
        "\n",
        "$$\n",
        "J = \\mathrm{E}_{\\tau\\sim P_\\pi} \\left[ G(\\tau) | S_{t=0}=s_0 \\right],\\quad G(\\tau)=\\sum_{t=1}^T r(s_t,a_t).\n",
        "$$\n",
        "\n",
        "Policy gradient methods in RL approximate directly the policy $\\pi\\approx\\pi_\\theta$ using a variational ansatz, parametrized by the unknown parameters $\\theta$. The goal is then to find those optimal parameters $\\theta$, which optimize the RL objective $J(\\theta)$. To define an update rule for $\\theta$, we may use gradient ascent. This requires us to evaluate the gradient of the RL objective w.r.t. the parameters $\\theta$:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\mathrm{E}_{\\tau\\sim P_\\pi} \\left[ \\sum_{t=1}^T r(s_t,a_t) | S_{t=0}=s_0 \\right] \n",
        "= \\int\\mathrm{d}\\tau \\nabla_\\theta P_{\\pi_\\theta}(\\tau) G(\\tau).\n",
        "$$\n",
        "In a model-free setting, we don't have access to the transition probabilities $p(s'|s,a)$ and this requires us to be able to estimate the gradients from samples. This can be accomplished by noticing that $\\nabla_\\theta P_{\\pi_\\theta} = P_{\\pi_\\theta} \\nabla_\\theta \\log P_{\\pi_\\theta}$ (almost everywhere, i.e. up to a set of measure zero):\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\int\\mathrm{d}\\tau \\nabla_\\theta P_{\\pi_\\theta}(\\tau) G(\\tau) = \\int\\mathrm{d}\\tau P_{\\pi_\\theta}(\\tau) \\nabla_\\theta \\log P_{\\pi_\\theta}(\\tau) G(\\tau) = \\mathrm{E}_{\\tau\\sim P_\\pi} \\left[\\nabla_\\theta \\log P_{\\pi_\\theta}(\\tau) G(\\tau)\\right].\n",
        "$$\n",
        "Since the initial state distribution and the transition proabilities are independent of $\\theta$, using the definition of $P_{\\pi_\\theta}$, we see that $\\nabla_\\theta P_{\\pi_\\theta}(\\tau) = \\nabla_\\theta \\pi_\\theta(\\tau)$ where $\\pi_\\theta(\\tau) = \\prod_{t=1}^T \\pi(a_t|s_t)$. \n",
        "\n",
        "We can now use MC to estimate the gradients directly from a sample of trajectories $\\{\\tau_j\\}_{j=1}^N$:\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathrm{E}_{\\tau\\sim P_\\pi} \\left[\\nabla_\\theta \\log \\pi_\\theta(\\tau) G(\\tau)\\right]\n",
        "\\approx \\frac{1}{N}\\sum_{j=1}^N \\nabla_\\theta \\log \\pi_\\theta(\\tau_j) G(\\tau_j).\n",
        "$$\n",
        "\n",
        "To alleviate the problem with the large variance of the gradient estimate, one can introduce a baseline $b$. The PG update then rakes the form\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta)\n",
        "\\approx \\frac{1}{N}\\sum_{j=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a^j_t|s^j_t) \\left[\\sum_{t'=t}^T r(a^j_{t'}|s^j_{t'})) - b\\right].\n",
        "$$\n",
        "\n",
        "One can show that adding this baseline term does not change the gradient of the objective.\n",
        "\n",
        "The corresponding gradient ascent update rule reads as\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta),\n",
        "$$\n",
        "for some step size (or learning rate) $\\alpha$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phj-0zwt2tlQ"
      },
      "source": [
        "## Qubit Environment\n",
        "\n",
        "Let us define the qubit environment that models the action of gates on the state of the two-level system (2LS).\n",
        "\n",
        "### Basic Definitions\n",
        "\n",
        "The state of a qubit $|\\psi\\rangle\\in\\mathbb{C}^2$ is modeled by a two-dimensional complex-valued vector with unit norm: $\\langle\\psi|\\psi\\rangle:=\\sqrt{|\\psi_1|^2+|\\psi_2|^2}=1$. Every qubit state is uniquely described by two angles $\\theta\\in[0,\\pi]$ and $\\varphi\\in[0,2\\pi)$:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "|\\psi\\rangle=\n",
        "\\begin{pmatrix}\n",
        "\\psi_1 \\\\ \\psi_2\n",
        "\\end{pmatrix}=\n",
        "\\mathrm{e}^{i\\alpha}\n",
        "\\begin{pmatrix}\n",
        "\\cos\\frac{\\theta}{2} \\\\\n",
        "\\mathrm{e}^{i\\varphi}\\sin\\frac{\\theta}{2}\n",
        "\\end{pmatrix}\n",
        "\\end{eqnarray}\n",
        "\n",
        "The overall phase $\\alpha$ of a single quantum state has no physical meaning.\n",
        "Thus, any qubit state can be pictured as an arrow on the unit sphere (called the Bloch sphere) with angular coordinates $(\\theta,\\varphi)$. \n",
        "\n",
        "To operate on qubits, we use quantum gates. Quantum gates are represented as unitary transformations $U\\in \\mathrm{U(2)}$, where $\\mathrm{U(2)}$ is the unitary group. Gates act on qubit states by matrix multiplication to transform an input state $|\\psi\\rangle$ to the output state $|\\psi'\\rangle$: $|\\psi'\\rangle=U|\\psi\\rangle$. For this problem, we consider four gates\n",
        "\n",
        "\\begin{equation}\n",
        "U_0=\\boldsymbol{1},\\qquad \n",
        "U_x=\\mathrm{exp}(-i\\delta t \\sigma^x/2),\\qquad\n",
        "U_y=\\mathrm{exp}(-i\\delta t \\sigma^y/2),\\qquad \n",
        "U_z=\\mathrm{exp}(-i\\delta t \\sigma^z/2),\n",
        "\\end{equation}\n",
        "\n",
        "where $\\delta t$ is a fixed time step, $\\mathrm{exp}(\\cdot)$ is the matrix exponential, $\\boldsymbol{1}$ is the identity, and the Pauli matrices are defined as\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{1}=\\begin{pmatrix}\n",
        "1 & 0 \\\\ 0 & 1\n",
        "\\end{pmatrix}\n",
        ",\\qquad\n",
        "\\sigma^x=\\begin{pmatrix}\n",
        "0 & 1 \\\\ 1 & 0\n",
        "\\end{pmatrix}\n",
        ",\\qquad\n",
        "\\sigma^y=\\begin{pmatrix}\n",
        "0 & -i \\\\ i & 0\n",
        "\\end{pmatrix}\n",
        ",\\ \\qquad\n",
        "\\sigma^z=\\begin{pmatrix}\n",
        "1 & 0 \\\\ 0 & -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "To determine if a qubit, described by the state $|\\psi\\rangle$, is in a desired target state $|\\psi_\\mathrm{target}\\rangle$, we compute the fidelity\n",
        "\n",
        "\\begin{eqnarray}\n",
        "F=|\\langle\\psi_\\mathrm{target}|\\psi\\rangle|^2 = |(\\psi_\\mathrm{target})^\\ast_1 \\psi_1 + (\\psi_\\mathrm{target})^\\ast_2 \\psi_2|^2,\\qquad F\\in[0,1]\n",
        "\\end{eqnarray}\n",
        "\n",
        "where $\\ast$ stands for complex conjugation. Physically, the fidelity corresponds to the angle between the arrows representing the qubit state on the Bloch sphere (we want to maximize the fidelity but minimize the angle between the states).\n",
        "\n",
        "### Constructing the Qubit Environment\n",
        "\n",
        "Now, let us define an episodic RL environment, which contains the laws of physics that govern the dynamics of the qubit (i.e. the application of the gate operations to the qubit state). Our RL agent will later interact with this environment to learn how to control the qubit to bring it from an initial state to a prescribed target state. \n",
        "\n",
        "We define the RL states $s=(\\theta,\\varphi)$ as an array containing the Bloch sphere angles of the quantum state. Each step within an episode, the agent can choose to apply one out of the actions, corresponding to the four gates $(\\boldsymbol{1},U_x,U_y,U_z)$. We use the instantaneous fidelity w.r.t. the target state as a reward: $r_t=F=|\\langle\\psi_\\ast|\\psi(t)\\rangle|^2$: \n",
        "\n",
        "**state space:** $\\mathcal{S} = \\{(\\theta,\\varphi)|\\theta\\in[0,\\pi],\\varphi\\in[0,2\\pi)\\}$. There are no well-defined terminal state in this task. Instead, we consider a fixed number of time steps, after which the episode terminates deterministically. The target state (i.e. the qubit state we want to prepare) is $|\\psi_\\mathrm{target}\\rangle=(1,0)^t$: it has the Bloch sphere coordinates $s_\\mathrm{target}=(0,0)$.\n",
        "\n",
        "**action space:** $\\mathcal{A} = \\{\\boldsymbol{1},U_x,U_y,U_z\\}$. Actions act on RL states as follows: \n",
        "1. if the current state is $s=(\\theta,\\varphi)$, we first create the quantums state $|\\psi(s)\\rangle$; \n",
        "2. we apply the gate $U_a$ corresponding to action $a$ to the quantum state, and obtain the new quantum state $|\\psi(s')\\rangle = U_a|\\psi(s)\\rangle$. \n",
        "3. last, we compute the Bloch sphere coordinates which define the next state $s'=(\\theta',\\varphi')$, using the Bloch sphere parametrization for qubits given above.\n",
        "Note that all actions are allowed from every state. \n",
        "\n",
        "\n",
        "**reward space:** $\\mathcal{R}=[0,1]$. We use the fidelity between the next state $s'$ and the terminal state $s_\\mathrm{target}$ as a reward at every episode step: \n",
        "\n",
        "$$r(s,s',a)= F = |\\langle\\psi_\\mathrm{target}|U_a|\\psi(s)\\rangle|^2=|\\langle\\psi_\\mathrm{target}|\\psi(s')\\rangle|^2$$\n",
        "\n",
        "for all states $s,s'\\in\\mathcal{S}$ and actions $a\\in\\mathcal{A}$. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QubitEnv():\n",
        "    \"\"\"\n",
        "    Gym style environment for RL. You may also inherit the class structure from OpenAI Gym. \n",
        "    Parameters:\n",
        "        n_time_steps:   int\n",
        "                        Total number of time steps within each episode\n",
        "        seed:   int\n",
        "                seed of the RNG (for reproducibility)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_time_steps, seed):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        self.n_time_steps = n_time_steps\n",
        "        \n",
        "        \n",
        "        ### define action space variables\n",
        "        self.n_actions = 4 # action space size\n",
        "        delta_t = 2*np.pi/n_time_steps # set a value for the time step\n",
        "        # define Pauli matrices\n",
        "        Id     =np.array([[1.0,0.0  ], [0.0 ,+1.0]])\n",
        "        sigma_x=np.array([[0.0,1.0  ], [1.0 , 0.0]])\n",
        "        sigma_y=np.array([[0.0,-1.0j], [1.0j, 0.0]])\n",
        "        sigma_z=np.array([[1.0,0.0  ], [0.0 ,-1.0]])\n",
        "        \n",
        "        self.action_space=[]\n",
        "        for generator in [Id, sigma_x, sigma_y, sigma_z]:\n",
        "            self.action_space.append( expm(-1j*delta_t*generator) )\n",
        "        \n",
        "        self.actions = np.array([0,1,2,3])\n",
        "        \n",
        "        \n",
        "        ### define state space variables\n",
        "        self.S_target = np.array([0.0,0.0])\n",
        "        self.psi_target = self.RL_to_qubit_state(self.S_target)\n",
        "        \n",
        "        \n",
        "        # set seed\n",
        "        self.set_seed(seed)\n",
        "        self.reset()\n",
        "    \n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Interface between environment and agent. Performs one step in the environemnt.\n",
        "        Parameters:\n",
        "            action: int\n",
        "                    the index of the respective action in the action array\n",
        "        Returns:\n",
        "            output: ( object, float, bool)\n",
        "                    information provided by the environment about its current state:\n",
        "                    (state, reward, done)\n",
        "        \"\"\"\n",
        "\n",
        "        # apply gate to quantum state\n",
        "        self.psi = self.action_space[action].dot(self.psi)\n",
        "        \n",
        "        # compute RL state\n",
        "        self.state = self.qubit_to_RL_state(self.psi)\n",
        "        \n",
        "        # compute reward\n",
        "        reward = np.abs( self.psi_target.conj().dot(self.psi)  )**2\n",
        "        \n",
        "        \n",
        "        # check if state is terminal\n",
        "        done=False\n",
        "        \n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "    \n",
        "    \n",
        "    def set_seed(self,seed=0):\n",
        "        \"\"\"\n",
        "        Sets the seed of the RNG.\n",
        "        \n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def reset(self, random=True):\n",
        "        \"\"\"\n",
        "        Resets the environment to its initial values.\n",
        "        Returns:\n",
        "            state:  object\n",
        "                    the initial state of the environment\n",
        "            random: bool\n",
        "                    controls whether the initial state is a random state on the sphere or a fixed initial state.\n",
        "        \"\"\"\n",
        "        \n",
        "        if random:\n",
        "            theta = np.pi*np.random.uniform(0.0,1.0)\n",
        "            phi = 2*np.pi*np.random.uniform(0.0,1.0)\n",
        "        else:\n",
        "            # start from south pole of Bloch sphere\n",
        "            theta=np.pi\n",
        "            phi=0.0\n",
        "        \n",
        "        self.state=np.array([theta,phi])\n",
        "        self.psi=self.RL_to_qubit_state(self.state)\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    \n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Plots the state of the environment. For visulization purposes only. Feel free to ignore. \n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def RL_to_qubit_state(self,s):\n",
        "        \"\"\"\n",
        "        Take as input the RL state s, and return the quantum state |psi>\n",
        "        \"\"\"\n",
        "        theta, phi = s\n",
        "        psi = np.array([np.cos(0.5*theta), np.exp(1j*phi)*np.sin(0.5*theta)] )\n",
        "        return psi\n",
        "    \n",
        "    \n",
        "    def qubit_to_RL_state(self,psi):\n",
        "        \"\"\"\n",
        "        Take as input the RL state s, and return the quantum state |psi>\n",
        "        \"\"\"\n",
        "        # take away unphysical global phase\n",
        "        alpha = np.angle(psi[0])\n",
        "        psi_new = np.exp(-1j*alpha) * psi \n",
        "        \n",
        "        # find Bloch sphere angles\n",
        "        theta = 2.0*np.arccos(psi_new[0]).real\n",
        "        phi = np.angle(psi_new[1])\n",
        "        \n",
        "        return np.array([theta, phi])"
      ],
      "metadata": {
        "id": "STFbyoqq4frU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test how the qubit environment works."
      ],
      "metadata": {
        "id": "a1EyMAt-4i7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "29-YtNb02tlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940d847a-b3b5-4281-ef53-5592373a3a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. s=[1.89 3.42], a=1, r=0.317401, s'=[ 1.94 -2.93]\n",
            "\n",
            "1. s=[ 1.94 -2.93], a=3, r=0.317401, s'=[ 1.94 -2.72]\n",
            "\n",
            "2. s=[ 1.94 -2.72], a=1, r=0.282162, s'=[ 2.02 -2.81]\n",
            "\n",
            "3. s=[ 2.02 -2.81], a=2, r=0.37539, s'=[ 1.82 -2.83]\n",
            "\n",
            "4. s=[ 1.82 -2.83], a=0, r=0.37539, s'=[ 1.82 -2.83]\n",
            "\n",
            "5. s=[ 1.82 -2.83], a=3, r=0.37539, s'=[ 1.82 -2.62]\n",
            "\n",
            "6. s=[ 1.82 -2.62], a=2, r=0.465631, s'=[ 1.64 -2.64]\n",
            "\n",
            "7. s=[ 1.64 -2.64], a=0, r=0.465631, s'=[ 1.64 -2.64]\n",
            "\n",
            "8. s=[ 1.64 -2.64], a=0, r=0.465631, s'=[ 1.64 -2.64]\n",
            "\n",
            "9. s=[ 1.64 -2.64], a=0, r=0.465631, s'=[ 1.64 -2.64]\n",
            "\n",
            "10. s=[ 1.64 -2.64], a=2, r=0.557374, s'=[ 1.46 -2.64]\n",
            "\n",
            "11. s=[ 1.46 -2.64], a=1, r=0.506358, s'=[ 1.56 -2.63]\n",
            "\n",
            "12. s=[ 1.56 -2.63], a=2, r=0.596708, s'=[ 1.38 -2.62]\n",
            "\n",
            "13. s=[ 1.38 -2.62], a=3, r=0.596708, s'=[ 1.38 -2.41]\n",
            "\n",
            "14. s=[ 1.38 -2.41], a=3, r=0.596708, s'=[ 1.38 -2.2 ]\n",
            "\n",
            "15. s=[ 1.38 -2.2 ], a=2, r=0.654397, s'=[ 1.26 -2.15]\n",
            "\n",
            "16. s=[ 1.26 -2.15], a=0, r=0.654397, s'=[ 1.26 -2.15]\n",
            "\n",
            "17. s=[ 1.26 -2.15], a=1, r=0.568402, s'=[ 1.43 -2.13]\n",
            "\n",
            "18. s=[ 1.43 -2.13], a=1, r=0.479417, s'=[ 1.61 -2.12]\n",
            "\n",
            "19. s=[ 1.61 -2.12], a=1, r=0.391332, s'=[ 1.79 -2.14]\n",
            "\n",
            "20. s=[ 1.79 -2.14], a=1, r=0.307996, s'=[ 1.96 -2.17]\n",
            "\n",
            "21. s=[ 1.96 -2.17], a=0, r=0.307996, s'=[ 1.96 -2.17]\n",
            "\n",
            "22. s=[ 1.96 -2.17], a=1, r=0.233052, s'=[ 2.13 -2.24]\n",
            "\n",
            "23. s=[ 2.13 -2.24], a=0, r=0.233052, s'=[ 2.13 -2.24]\n",
            "\n",
            "24. s=[ 2.13 -2.24], a=3, r=0.233052, s'=[ 2.13 -2.03]\n",
            "\n",
            "25. s=[ 2.13 -2.03], a=0, r=0.233052, s'=[ 2.13 -2.03]\n",
            "\n",
            "26. s=[ 2.13 -2.03], a=3, r=0.233052, s'=[ 2.13 -1.82]\n",
            "\n",
            "27. s=[ 2.13 -1.82], a=1, r=0.153658, s'=[ 2.34 -1.86]\n",
            "\n",
            "28. s=[ 2.34 -1.86], a=2, r=0.182735, s'=[ 2.26 -2.04]\n",
            "\n",
            "29. s=[ 2.26 -2.04], a=3, r=0.182735, s'=[ 2.26 -1.83]\n",
            "\n",
            "30. s=[ 2.26 -1.83], a=3, r=0.182735, s'=[ 2.26 -1.62]\n",
            "\n",
            "31. s=[ 2.26 -1.62], a=0, r=0.182735, s'=[ 2.26 -1.62]\n",
            "\n",
            "32. s=[ 2.26 -1.62], a=2, r=0.193351, s'=[ 2.23 -1.78]\n",
            "\n",
            "33. s=[ 2.23 -1.78], a=3, r=0.193351, s'=[ 2.23 -1.57]\n",
            "\n",
            "34. s=[ 2.23 -1.57], a=0, r=0.193351, s'=[ 2.23 -1.57]\n",
            "\n",
            "35. s=[ 2.23 -1.57], a=1, r=0.117943, s'=[ 2.44 -1.57]\n",
            "\n",
            "36. s=[ 2.44 -1.57], a=3, r=0.117943, s'=[ 2.44 -1.37]\n",
            "\n",
            "37. s=[ 2.44 -1.37], a=1, r=0.060646, s'=[ 2.64 -1.29]\n",
            "\n",
            "38. s=[ 2.64 -1.29], a=3, r=0.060646, s'=[ 2.64 -1.08]\n",
            "\n",
            "39. s=[ 2.64 -1.08], a=3, r=0.060646, s'=[ 2.64 -0.87]\n",
            "\n",
            "40. s=[ 2.64 -0.87], a=2, r=0.038334, s'=[ 2.75 -1.26]\n",
            "\n",
            "41. s=[ 2.75 -1.26], a=3, r=0.038334, s'=[ 2.75 -1.05]\n",
            "\n",
            "42. s=[ 2.75 -1.05], a=0, r=0.038334, s'=[ 2.75 -1.05]\n",
            "\n",
            "43. s=[ 2.75 -1.05], a=1, r=0.013792, s'=[ 2.91 -0.61]\n",
            "\n",
            "44. s=[ 2.91 -0.61], a=1, r=0.0105, s'=[2.94 0.36]\n",
            "\n",
            "45. s=[2.94 0.36], a=1, r=0.028602, s'=[2.8  0.96]\n",
            "\n",
            "46. s=[2.8  0.96], a=3, r=0.028602, s'=[2.8  1.17]\n",
            "\n",
            "47. s=[2.8  1.17], a=0, r=0.028602, s'=[2.8  1.17]\n",
            "\n",
            "48. s=[2.8  1.17], a=3, r=0.028602, s'=[2.8  1.38]\n",
            "\n",
            "49. s=[2.8  1.38], a=2, r=0.032315, s'=[2.78 1.96]\n",
            "\n",
            "50. s=[2.78 1.96], a=0, r=0.032315, s'=[2.78 1.96]\n",
            "\n",
            "51. s=[2.78 1.96], a=3, r=0.032315, s'=[2.78 2.17]\n",
            "\n",
            "52. s=[2.78 2.17], a=3, r=0.032315, s'=[2.78 2.38]\n",
            "\n",
            "53. s=[2.78 2.38], a=2, r=0.069103, s'=[2.61 2.64]\n",
            "\n",
            "54. s=[2.61 2.64], a=3, r=0.069103, s'=[2.61 2.85]\n",
            "\n",
            "55. s=[2.61 2.85], a=2, r=0.128997, s'=[2.41 2.92]\n",
            "\n",
            "56. s=[2.41 2.92], a=3, r=0.128997, s'=[2.41 3.13]\n",
            "\n",
            "57. s=[2.41 3.13], a=0, r=0.128997, s'=[2.41 3.13]\n",
            "\n",
            "58. s=[2.41 3.13], a=2, r=0.206791, s'=[2.2  3.13]\n",
            "\n",
            "59. s=[2.2  3.13], a=0, r=0.206791, s'=[2.2  3.13]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# set seed of rng (for reproducibility of the results)\n",
        "n_time_steps = 60 # steps of each episode\n",
        "\n",
        "# create environment and reset it to a random initial state\n",
        "env=QubitEnv(n_time_steps,seed)\n",
        "env.reset(random=True)\n",
        "\n",
        "done=False\n",
        "j=0\n",
        "while j < n_time_steps:\n",
        "    \n",
        "    # pick a random action\n",
        "    action=np.random.choice([0,1,2,3]) # equiprobable policy\n",
        "    \n",
        "    # take an environment step\n",
        "    state=env.state.copy()\n",
        "    state_p, reward, done = env.step(action)\n",
        "    \n",
        "    print(\"{}. s={}, a={}, r={}, s'={}\\n\".format(j, state, action, np.round(reward,6), state_p))\n",
        "    \n",
        "    j+=1\n",
        "    \n",
        "    if done:\n",
        "        print('\\nreached terminal state!')\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNPlNyN_2tlT"
      },
      "source": [
        "## Policy Gradient Implementation\n",
        "\n",
        "The implementation of the PG algorithm proceeds as follows:\n",
        "\n",
        "1. Define the a SoftMax model for the discrete policy $\\pi_\\theta$.\n",
        "2. Define the pseudo loss function to easily compute $\\nabla_\\theta J(\\theta)$.\n",
        "3. Define generalized gradient descent optimizer.\n",
        "4. Define the PG training loop and train the policy.\n",
        "\n",
        "*Note:* if you are familiar with solving the MNIST problem, you will recognize many of the steps used to construct and train the neural network. What is different here is the training algorithm.\n",
        "\n",
        "### Define a SoftMax model for the discrete policy $\\pi_\\theta$\n",
        "\n",
        "Use JAX to construct a feed-forward fully-connected deep neural network with neuron acrchitecture $(M_s, 512, 256, |\\mathcal{A}|)$, where there are $512$ ($256$) neurons in the first (second) hidden layer, respectively, and $M_s$ and $|\\mathcal{A}|$ define the input and output sizes.\n",
        "\n",
        "The input data into the neural network should have the shape `input_shape = (-1, n_time_steps, M_s)`, where `M_s` is the number of features/components in the RL state $s=(\\theta,\\varphi)$. The output data should have the shape `output_shape = (-1, n_time_steps, abs_A)`, where `abs_A`$=|\\mathcal{A}|$. In this way, we can use the neural network to process simultaneously all time steps and MC samples, generated in a single training iteration. \n",
        "\n",
        "Check explicitly the output shape and test that the network runs on some fake data (e.g. a small batch of vectors of ones with the appropriate shape). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X8s8-3P12tlT",
        "outputId": "82d46569-7732-4ebb-b0a1-f55ed6bd1076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "output shape of the policy network is (-1, 60, 4).\n",
            "\n",
            "(3, 60, 4)\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp # jax's numpy version with GPU support\n",
        "from jax import random # used to define a RNG key to control the random input in JAX\n",
        "from jax.example_libraries import stax # neural network library\n",
        "from jax.example_libraries.stax import Dense, Relu, LogSoftmax # neural network layers\n",
        "\n",
        "# set key for the RNG (see JAX docs)\n",
        "rng = random.PRNGKey(seed)\n",
        "\n",
        "# define functions which initialize the parameters and evaluate the model\n",
        "initialize_params, predict = stax.serial(\n",
        "                                            ### fully connected DNN\n",
        "                                            Dense(512), # 512 hidden neurons\n",
        "                                            Relu,\n",
        "                                            Dense(256), # 256 hidden neurons\n",
        "                                            Relu,\n",
        "                                            Dense(env.n_actions), # 4 output neurons\n",
        "                                            LogSoftmax # NB: computes the log-probability\n",
        "                                        )\n",
        "\n",
        "# initialize the model parameters\n",
        "input_shape = (-1,env.n_time_steps,2) # -1: number of MC points, number of time steps, size of state vector\n",
        "output_shape, inital_params = initialize_params(rng, input_shape) # fcc layer 28x28 pixes in each image\n",
        "\n",
        "print('\\noutput shape of the policy network is {}.\\n'.format(output_shape))\n",
        "\n",
        "\n",
        "# test network\n",
        "states=np.ones((3,env.n_time_steps,2), dtype=np.float32)\n",
        "\n",
        "predictions = predict(inital_params, states)\n",
        "# check the output shape\n",
        "print(predictions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "343Vdvxv2tlT"
      },
      "source": [
        "### Define the pseudo loss function to easily compute $\\nabla_\\theta J(\\theta)$\n",
        "\n",
        "REINFORCE allows to define a scalar pseudoloss function, whose gradients give $\\nabla_\\theta J(\\theta)$. Note that this pseudoloss does ***NOT*** correspond to the RL objective $J(\\theta)$: the difference stems from the fact that the two operations of taking the derivative and performing the MC approximation are not interchangeable (do you see why?).  \n",
        "\n",
        "$$\n",
        "J_\\mathrm{pseudo}(\\theta) = \n",
        "\\frac{1}{N}\\sum_{j=1}^N \\sum_{t=1}^T \\log \\pi_\\theta(a^j_t|s^j_t) \\left[\\sum_{t'=t}^T r(a^j_{t'}|s^j_{t'}) - b_t\\right],\\qquad \n",
        "b_t = \\frac{1}{N}\\sum_{j=1}^N G_t(\\tau_j).\n",
        "$$\n",
        "The baseline is a sample average of the reward-to-go (return) from time step $t$ onwards: $G_t(\\tau_j) = \\sum_{t'=t}^N r(s^j_{t'},s^j_{t'})$ .\n",
        "\n",
        "Because we will be doing gradient **a**scent, do **NOT** forget to add an extra minus sign to the output ot the pseudoloss (or else your agent will end up minimizing the return). \n",
        "\n",
        "Below, we also add an L2 regularizer to the pseudoloss function to prevent overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LmTcCdqQ2tlU"
      },
      "outputs": [],
      "source": [
        "### define loss and accuracy functions\n",
        "\n",
        "from jax import grad\n",
        "from jax.tree_util import tree_flatten # jax params are stored as nested tuples; use this to manipulate tuples\n",
        "\n",
        "\n",
        "def l2_regularizer(params, lmbda):\n",
        "    \"\"\"\n",
        "    Define l2 regularizer: $\\lambda \\ sum_j ||theta_j||^2 $ for every parameter in the model $\\theta_j$\n",
        "    \n",
        "    \"\"\"\n",
        "    return lmbda*jnp.sum(jnp.array([jnp.sum(jnp.abs(theta)**2) for theta in tree_flatten(params)[0] ]))\n",
        "\n",
        "\n",
        "def pseudo_loss(params, trajectory_batch):\n",
        "    \"\"\"\n",
        "    Define the pseudo loss function for policy gradient. \n",
        "    \n",
        "    params: object(jax pytree):\n",
        "        parameters of the deep policy network.\n",
        "    trajectory_batch: tuple (states, actions, returns) containing the RL states, actions and returns (not the rewards!): \n",
        "        states: np.array of size (N_MC, env.n_time_steps,2)\n",
        "        actions: np.array of size (N_MC, env.n_time_steps)\n",
        "        returns: np.array of size (N_MC, env.n_time_steps)\n",
        "    \n",
        "    Returns:\n",
        "        -J_{pseudo}(\\theta)\n",
        "\n",
        "    \"\"\"\n",
        "    # extract data from the batch\n",
        "    states, actions, returns = trajectory_batch\n",
        "    # compute policy predictions\n",
        "    preds = predict(params, states)\n",
        "    # combute the baseline\n",
        "    baseline = jnp.mean(returns, axis=0)\n",
        "    # select those values of the policy along the action trajectory\n",
        "    preds_select = jnp.take_along_axis(preds, jnp.expand_dims(actions, axis=2), axis=2).squeeze()\n",
        "    # return negative pseudo loss function (want to maximize reward with gradient DEscent)\n",
        "    return -jnp.mean(jnp.sum(preds_select * (returns - baseline) )) + l2_regularizer(params, 0.001)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quxCV2lm2tlU"
      },
      "source": [
        "### Define generalized gradient descent optimizer\n",
        "\n",
        "Define the optimizer and the `update` function which computes the gradient o the pseudo-loss function and performs the update. \n",
        "\n",
        "We use the Adam optimizer here with `step_size = 0.001` and the rest of the parameters have default values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nVKex_mp2tlU"
      },
      "outputs": [],
      "source": [
        "### define generalized gradient descent optimizer and a function to update model parameters\n",
        "\n",
        "from jax.example_libraries import optimizers # gradient descent optimizers\n",
        "from jax import jit\n",
        "\n",
        "step_size = 0.001 # step size or learning rate \n",
        "\n",
        "# compute optimizer functions\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "\n",
        "\n",
        "# define function which updates the parameters using the change computed by the optimizer\n",
        "@jit # Just In Time compilation speeds up the code; requires to use jnp everywhere; remove when debugging\n",
        "def update(i, opt_state, batch):\n",
        "    \"\"\"\n",
        "    i: int,\n",
        "        counter to count how many update steps we have performed\n",
        "    opt_state: object,\n",
        "        the state of the optimizer\n",
        "    batch: np.array\n",
        "        batch containing the data used to update the model\n",
        "        \n",
        "    Returns: \n",
        "    opt_state: object,\n",
        "        the new state of the optimizer\n",
        "        \n",
        "    \"\"\"\n",
        "    # get current parameters of the model\n",
        "    current_params = get_params(opt_state)\n",
        "    # compute gradients\n",
        "    grad_params = grad(pseudo_loss)(current_params, batch)\n",
        "    # use the optimizer to perform the update using opt_update\n",
        "    return opt_update(i, grad_params, opt_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVYUTo0-2tlU"
      },
      "source": [
        "### Define the PG training loop and train the policy\n",
        "\n",
        "Finally, we implement the REINFORCE algorithm for policy gradient. Follow the steps below:\n",
        "\n",
        "1. Preallocate variables\n",
        "    * Define the number of episodes `N_episodes`, and the batch size `N_MC`.\n",
        "    * Preallocate arrays for the current `state`, and the `states`, `actions`, `returns` triple which defines the trajectory batch. \n",
        "    * Preallocate arrays to compute the `mean_final_reward`, `std_final_reward`, `min_final_reward`, and , `max_final_reward`.\n",
        "2. Initialize the optimizer using the `opt_init` function. \n",
        "3. Loop over the episodes; for every episode:\n",
        "\n",
        "    3.1 get the current Network parameters\n",
        "    \n",
        "    3.2 loop to collect MC samples\n",
        "        \n",
        "      3.2.1 reset the `env` and roll out the policy until the episode is over; collect the trajectory data\n",
        "    \n",
        "      3.2.2 compute the returns (rewards to go)\n",
        "    \n",
        "    3.3 compile the PG data into a trajctory batch\n",
        "    \n",
        "    3.4 use the `update` function to update the network parameters\n",
        "    \n",
        "    3.5 print instantaneous performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GDG845ox2tlV",
        "outputId": "1212c2d9-56db-49ec-a61c-dc7070fa2e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "episode 0 in 7.50 sec\n",
            "mean reward: 0.4893\n",
            "return standard deviation: 0.2976\n",
            "min return: 0.0093; max return: 0.9973\n",
            "\n",
            "episode 1 in 5.27 sec\n",
            "mean reward: 0.5089\n",
            "return standard deviation: 0.2988\n",
            "min return: 0.0197; max return: 0.9954\n",
            "\n",
            "episode 2 in 4.45 sec\n",
            "mean reward: 0.5773\n",
            "return standard deviation: 0.2473\n",
            "min return: 0.1093; max return: 0.9966\n",
            "\n",
            "episode 3 in 2.95 sec\n",
            "mean reward: 0.5914\n",
            "return standard deviation: 0.2809\n",
            "min return: 0.0190; max return: 0.9989\n",
            "\n",
            "episode 4 in 2.35 sec\n",
            "mean reward: 0.5910\n",
            "return standard deviation: 0.2662\n",
            "min return: 0.0206; max return: 0.9969\n",
            "\n",
            "episode 5 in 2.33 sec\n",
            "mean reward: 0.5786\n",
            "return standard deviation: 0.2607\n",
            "min return: 0.0034; max return: 0.9978\n",
            "\n",
            "episode 6 in 2.32 sec\n",
            "mean reward: 0.5554\n",
            "return standard deviation: 0.2453\n",
            "min return: 0.0214; max return: 0.9538\n",
            "\n",
            "episode 7 in 2.32 sec\n",
            "mean reward: 0.6352\n",
            "return standard deviation: 0.2590\n",
            "min return: 0.0764; max return: 0.9860\n",
            "\n",
            "episode 8 in 2.38 sec\n",
            "mean reward: 0.6361\n",
            "return standard deviation: 0.2454\n",
            "min return: 0.1240; max return: 0.9992\n",
            "\n",
            "episode 9 in 2.36 sec\n",
            "mean reward: 0.6827\n",
            "return standard deviation: 0.2230\n",
            "min return: 0.1135; max return: 0.9809\n",
            "\n",
            "episode 10 in 2.38 sec\n",
            "mean reward: 0.7056\n",
            "return standard deviation: 0.1931\n",
            "min return: 0.2164; max return: 0.9914\n",
            "\n",
            "episode 11 in 2.39 sec\n",
            "mean reward: 0.6368\n",
            "return standard deviation: 0.1979\n",
            "min return: 0.1794; max return: 0.9580\n",
            "\n",
            "episode 12 in 2.39 sec\n",
            "mean reward: 0.7426\n",
            "return standard deviation: 0.1681\n",
            "min return: 0.2688; max return: 0.9981\n",
            "\n",
            "episode 13 in 2.39 sec\n",
            "mean reward: 0.7417\n",
            "return standard deviation: 0.1958\n",
            "min return: 0.0357; max return: 0.9803\n",
            "\n",
            "episode 14 in 2.34 sec\n",
            "mean reward: 0.7254\n",
            "return standard deviation: 0.1722\n",
            "min return: 0.2919; max return: 0.9913\n",
            "\n",
            "episode 15 in 2.34 sec\n",
            "mean reward: 0.7686\n",
            "return standard deviation: 0.1489\n",
            "min return: 0.3449; max return: 0.9972\n",
            "\n",
            "episode 16 in 2.35 sec\n",
            "mean reward: 0.8075\n",
            "return standard deviation: 0.1217\n",
            "min return: 0.5141; max return: 0.9969\n",
            "\n",
            "episode 17 in 2.34 sec\n",
            "mean reward: 0.7815\n",
            "return standard deviation: 0.1565\n",
            "min return: 0.2347; max return: 0.9899\n",
            "\n",
            "episode 18 in 2.36 sec\n",
            "mean reward: 0.8202\n",
            "return standard deviation: 0.1215\n",
            "min return: 0.4412; max return: 0.9981\n",
            "\n",
            "episode 19 in 2.40 sec\n",
            "mean reward: 0.8200\n",
            "return standard deviation: 0.1201\n",
            "min return: 0.4442; max return: 0.9933\n",
            "\n",
            "episode 20 in 2.40 sec\n",
            "mean reward: 0.8247\n",
            "return standard deviation: 0.1331\n",
            "min return: 0.4341; max return: 0.9973\n",
            "\n",
            "episode 21 in 2.35 sec\n",
            "mean reward: 0.8259\n",
            "return standard deviation: 0.1106\n",
            "min return: 0.5256; max return: 0.9958\n",
            "\n",
            "episode 22 in 2.33 sec\n",
            "mean reward: 0.8375\n",
            "return standard deviation: 0.1157\n",
            "min return: 0.5116; max return: 0.9776\n",
            "\n",
            "episode 23 in 2.34 sec\n",
            "mean reward: 0.8439\n",
            "return standard deviation: 0.0963\n",
            "min return: 0.5378; max return: 0.9976\n",
            "\n",
            "episode 24 in 2.40 sec\n",
            "mean reward: 0.8498\n",
            "return standard deviation: 0.1002\n",
            "min return: 0.5566; max return: 0.9941\n",
            "\n",
            "episode 25 in 2.38 sec\n",
            "mean reward: 0.8267\n",
            "return standard deviation: 0.1226\n",
            "min return: 0.4926; max return: 0.9997\n",
            "\n",
            "episode 26 in 2.38 sec\n",
            "mean reward: 0.8559\n",
            "return standard deviation: 0.0764\n",
            "min return: 0.6129; max return: 0.9978\n",
            "\n",
            "episode 27 in 2.37 sec\n",
            "mean reward: 0.8717\n",
            "return standard deviation: 0.0935\n",
            "min return: 0.5286; max return: 0.9961\n",
            "\n",
            "episode 28 in 2.38 sec\n",
            "mean reward: 0.8819\n",
            "return standard deviation: 0.0845\n",
            "min return: 0.5874; max return: 0.9952\n",
            "\n",
            "episode 29 in 2.75 sec\n",
            "mean reward: 0.8790\n",
            "return standard deviation: 0.0861\n",
            "min return: 0.5702; max return: 0.9941\n",
            "\n",
            "episode 30 in 2.38 sec\n",
            "mean reward: 0.8964\n",
            "return standard deviation: 0.0731\n",
            "min return: 0.7193; max return: 0.9974\n",
            "\n",
            "episode 31 in 2.40 sec\n",
            "mean reward: 0.8894\n",
            "return standard deviation: 0.0741\n",
            "min return: 0.7020; max return: 0.9874\n",
            "\n",
            "episode 32 in 2.38 sec\n",
            "mean reward: 0.8907\n",
            "return standard deviation: 0.0920\n",
            "min return: 0.6003; max return: 0.9902\n",
            "\n",
            "episode 33 in 2.44 sec\n",
            "mean reward: 0.9045\n",
            "return standard deviation: 0.0632\n",
            "min return: 0.7392; max return: 0.9955\n",
            "\n",
            "episode 34 in 2.40 sec\n",
            "mean reward: 0.9150\n",
            "return standard deviation: 0.0583\n",
            "min return: 0.7254; max return: 0.9975\n",
            "\n",
            "episode 35 in 2.38 sec\n",
            "mean reward: 0.9087\n",
            "return standard deviation: 0.0670\n",
            "min return: 0.7047; max return: 0.9919\n",
            "\n",
            "episode 36 in 2.41 sec\n",
            "mean reward: 0.9162\n",
            "return standard deviation: 0.0598\n",
            "min return: 0.6571; max return: 0.9944\n",
            "\n",
            "episode 37 in 3.83 sec\n",
            "mean reward: 0.9000\n",
            "return standard deviation: 0.0762\n",
            "min return: 0.6631; max return: 0.9999\n",
            "\n",
            "episode 38 in 2.40 sec\n",
            "mean reward: 0.9161\n",
            "return standard deviation: 0.0627\n",
            "min return: 0.6996; max return: 0.9831\n",
            "\n",
            "episode 39 in 2.34 sec\n",
            "mean reward: 0.9187\n",
            "return standard deviation: 0.0566\n",
            "min return: 0.6638; max return: 0.9947\n",
            "\n",
            "episode 40 in 2.32 sec\n",
            "mean reward: 0.9249\n",
            "return standard deviation: 0.0595\n",
            "min return: 0.7370; max return: 0.9996\n",
            "\n",
            "episode 41 in 2.40 sec\n",
            "mean reward: 0.9325\n",
            "return standard deviation: 0.0522\n",
            "min return: 0.7669; max return: 0.9999\n",
            "\n",
            "episode 42 in 2.35 sec\n",
            "mean reward: 0.9470\n",
            "return standard deviation: 0.0467\n",
            "min return: 0.8066; max return: 0.9997\n",
            "\n",
            "episode 43 in 2.83 sec\n",
            "mean reward: 0.9387\n",
            "return standard deviation: 0.0419\n",
            "min return: 0.8176; max return: 0.9998\n",
            "\n",
            "episode 44 in 2.83 sec\n",
            "mean reward: 0.9437\n",
            "return standard deviation: 0.0454\n",
            "min return: 0.7361; max return: 0.9934\n",
            "\n",
            "episode 45 in 2.38 sec\n",
            "mean reward: 0.9511\n",
            "return standard deviation: 0.0400\n",
            "min return: 0.8439; max return: 0.9989\n",
            "\n",
            "episode 46 in 2.37 sec\n",
            "mean reward: 0.9520\n",
            "return standard deviation: 0.0384\n",
            "min return: 0.8344; max return: 0.9993\n",
            "\n",
            "episode 47 in 2.34 sec\n",
            "mean reward: 0.9497\n",
            "return standard deviation: 0.0354\n",
            "min return: 0.8695; max return: 0.9990\n",
            "\n",
            "episode 48 in 2.39 sec\n",
            "mean reward: 0.9537\n",
            "return standard deviation: 0.0315\n",
            "min return: 0.8657; max return: 0.9985\n",
            "\n",
            "episode 49 in 2.40 sec\n",
            "mean reward: 0.9558\n",
            "return standard deviation: 0.0361\n",
            "min return: 0.8216; max return: 0.9998\n",
            "\n",
            "episode 50 in 2.36 sec\n",
            "mean reward: 0.9645\n",
            "return standard deviation: 0.0310\n",
            "min return: 0.8565; max return: 0.9984\n",
            "\n",
            "episode 51 in 2.38 sec\n",
            "mean reward: 0.9619\n",
            "return standard deviation: 0.0319\n",
            "min return: 0.8522; max return: 0.9992\n",
            "\n",
            "episode 52 in 2.36 sec\n",
            "mean reward: 0.9597\n",
            "return standard deviation: 0.0292\n",
            "min return: 0.8759; max return: 0.9967\n",
            "\n",
            "episode 53 in 2.38 sec\n",
            "mean reward: 0.9562\n",
            "return standard deviation: 0.0318\n",
            "min return: 0.8531; max return: 0.9997\n",
            "\n",
            "episode 54 in 2.39 sec\n",
            "mean reward: 0.9592\n",
            "return standard deviation: 0.0374\n",
            "min return: 0.7715; max return: 0.9989\n",
            "\n",
            "episode 55 in 2.38 sec\n",
            "mean reward: 0.9712\n",
            "return standard deviation: 0.0242\n",
            "min return: 0.8901; max return: 0.9988\n",
            "\n",
            "episode 56 in 2.39 sec\n",
            "mean reward: 0.9698\n",
            "return standard deviation: 0.0275\n",
            "min return: 0.8848; max return: 0.9994\n",
            "\n",
            "episode 57 in 2.34 sec\n",
            "mean reward: 0.9697\n",
            "return standard deviation: 0.0295\n",
            "min return: 0.8774; max return: 0.9997\n",
            "\n",
            "episode 58 in 2.36 sec\n",
            "mean reward: 0.9703\n",
            "return standard deviation: 0.0292\n",
            "min return: 0.8749; max return: 0.9999\n",
            "\n",
            "episode 59 in 2.34 sec\n",
            "mean reward: 0.9775\n",
            "return standard deviation: 0.0230\n",
            "min return: 0.8828; max return: 0.9999\n",
            "\n",
            "episode 60 in 2.35 sec\n",
            "mean reward: 0.9736\n",
            "return standard deviation: 0.0252\n",
            "min return: 0.8948; max return: 0.9999\n",
            "\n",
            "episode 61 in 2.40 sec\n",
            "mean reward: 0.9762\n",
            "return standard deviation: 0.0245\n",
            "min return: 0.8801; max return: 0.9997\n",
            "\n",
            "episode 62 in 2.38 sec\n",
            "mean reward: 0.9801\n",
            "return standard deviation: 0.0173\n",
            "min return: 0.9014; max return: 0.9999\n",
            "\n",
            "episode 63 in 2.36 sec\n",
            "mean reward: 0.9824\n",
            "return standard deviation: 0.0169\n",
            "min return: 0.9373; max return: 1.0000\n",
            "\n",
            "episode 64 in 2.37 sec\n",
            "mean reward: 0.9779\n",
            "return standard deviation: 0.0214\n",
            "min return: 0.8855; max return: 0.9997\n",
            "\n",
            "episode 65 in 2.36 sec\n",
            "mean reward: 0.9813\n",
            "return standard deviation: 0.0171\n",
            "min return: 0.9200; max return: 0.9996\n",
            "\n",
            "episode 66 in 2.40 sec\n",
            "mean reward: 0.9806\n",
            "return standard deviation: 0.0215\n",
            "min return: 0.9086; max return: 0.9993\n",
            "\n",
            "episode 67 in 2.40 sec\n",
            "mean reward: 0.9856\n",
            "return standard deviation: 0.0167\n",
            "min return: 0.9110; max return: 0.9998\n",
            "\n",
            "episode 68 in 2.38 sec\n",
            "mean reward: 0.9842\n",
            "return standard deviation: 0.0154\n",
            "min return: 0.9314; max return: 0.9999\n",
            "\n",
            "episode 69 in 2.47 sec\n",
            "mean reward: 0.9839\n",
            "return standard deviation: 0.0184\n",
            "min return: 0.9024; max return: 0.9999\n",
            "\n",
            "episode 70 in 2.43 sec\n",
            "mean reward: 0.9855\n",
            "return standard deviation: 0.0162\n",
            "min return: 0.9114; max return: 0.9999\n",
            "\n",
            "episode 71 in 2.40 sec\n",
            "mean reward: 0.9817\n",
            "return standard deviation: 0.0185\n",
            "min return: 0.9102; max return: 0.9992\n",
            "\n",
            "episode 72 in 2.41 sec\n",
            "mean reward: 0.9839\n",
            "return standard deviation: 0.0213\n",
            "min return: 0.8958; max return: 0.9999\n",
            "\n",
            "episode 73 in 2.40 sec\n",
            "mean reward: 0.9848\n",
            "return standard deviation: 0.0169\n",
            "min return: 0.9320; max return: 0.9997\n",
            "\n",
            "episode 74 in 2.39 sec\n",
            "mean reward: 0.9830\n",
            "return standard deviation: 0.0153\n",
            "min return: 0.9405; max return: 0.9999\n",
            "\n",
            "episode 75 in 2.42 sec\n",
            "mean reward: 0.9865\n",
            "return standard deviation: 0.0157\n",
            "min return: 0.9219; max return: 1.0000\n",
            "\n",
            "episode 76 in 2.39 sec\n",
            "mean reward: 0.9856\n",
            "return standard deviation: 0.0161\n",
            "min return: 0.9401; max return: 0.9999\n",
            "\n",
            "episode 77 in 2.41 sec\n",
            "mean reward: 0.9843\n",
            "return standard deviation: 0.0198\n",
            "min return: 0.9066; max return: 1.0000\n",
            "\n",
            "episode 78 in 2.40 sec\n",
            "mean reward: 0.9869\n",
            "return standard deviation: 0.0196\n",
            "min return: 0.8942; max return: 0.9999\n",
            "\n",
            "episode 79 in 2.39 sec\n",
            "mean reward: 0.9900\n",
            "return standard deviation: 0.0139\n",
            "min return: 0.9206; max return: 0.9999\n",
            "\n",
            "episode 80 in 2.40 sec\n",
            "mean reward: 0.9855\n",
            "return standard deviation: 0.0162\n",
            "min return: 0.9332; max return: 1.0000\n",
            "\n",
            "episode 81 in 2.45 sec\n",
            "mean reward: 0.9915\n",
            "return standard deviation: 0.0097\n",
            "min return: 0.9507; max return: 1.0000\n",
            "\n",
            "episode 82 in 2.42 sec\n",
            "mean reward: 0.9867\n",
            "return standard deviation: 0.0178\n",
            "min return: 0.9118; max return: 0.9997\n",
            "\n",
            "episode 83 in 2.42 sec\n",
            "mean reward: 0.9899\n",
            "return standard deviation: 0.0104\n",
            "min return: 0.9538; max return: 1.0000\n",
            "\n",
            "episode 84 in 2.41 sec\n",
            "mean reward: 0.9883\n",
            "return standard deviation: 0.0109\n",
            "min return: 0.9363; max return: 0.9998\n",
            "\n",
            "episode 85 in 2.42 sec\n",
            "mean reward: 0.9868\n",
            "return standard deviation: 0.0179\n",
            "min return: 0.9120; max return: 0.9998\n",
            "\n",
            "episode 86 in 2.46 sec\n",
            "mean reward: 0.9914\n",
            "return standard deviation: 0.0085\n",
            "min return: 0.9571; max return: 0.9997\n",
            "\n",
            "episode 87 in 2.42 sec\n",
            "mean reward: 0.9919\n",
            "return standard deviation: 0.0119\n",
            "min return: 0.9224; max return: 0.9998\n",
            "\n",
            "episode 88 in 2.42 sec\n",
            "mean reward: 0.9922\n",
            "return standard deviation: 0.0067\n",
            "min return: 0.9692; max return: 0.9998\n",
            "\n",
            "episode 89 in 2.42 sec\n",
            "mean reward: 0.9906\n",
            "return standard deviation: 0.0107\n",
            "min return: 0.9526; max return: 0.9999\n",
            "\n",
            "episode 90 in 2.35 sec\n",
            "mean reward: 0.9903\n",
            "return standard deviation: 0.0111\n",
            "min return: 0.9261; max return: 1.0000\n",
            "\n",
            "episode 91 in 2.37 sec\n",
            "mean reward: 0.9903\n",
            "return standard deviation: 0.0114\n",
            "min return: 0.9419; max return: 0.9999\n",
            "\n",
            "episode 92 in 2.37 sec\n",
            "mean reward: 0.9919\n",
            "return standard deviation: 0.0092\n",
            "min return: 0.9500; max return: 0.9999\n",
            "\n",
            "episode 93 in 2.39 sec\n",
            "mean reward: 0.9901\n",
            "return standard deviation: 0.0128\n",
            "min return: 0.9479; max return: 0.9999\n",
            "\n",
            "episode 94 in 2.43 sec\n",
            "mean reward: 0.9890\n",
            "return standard deviation: 0.0112\n",
            "min return: 0.9375; max return: 0.9999\n",
            "\n",
            "episode 95 in 2.40 sec\n",
            "mean reward: 0.9876\n",
            "return standard deviation: 0.0106\n",
            "min return: 0.9527; max return: 0.9999\n",
            "\n",
            "episode 96 in 2.38 sec\n",
            "mean reward: 0.9901\n",
            "return standard deviation: 0.0092\n",
            "min return: 0.9517; max return: 1.0000\n",
            "\n",
            "episode 97 in 2.37 sec\n",
            "mean reward: 0.9911\n",
            "return standard deviation: 0.0082\n",
            "min return: 0.9631; max return: 1.0000\n",
            "\n",
            "episode 98 in 2.44 sec\n",
            "mean reward: 0.9886\n",
            "return standard deviation: 0.0130\n",
            "min return: 0.9240; max return: 1.0000\n",
            "\n",
            "episode 99 in 2.37 sec\n",
            "mean reward: 0.9904\n",
            "return standard deviation: 0.0085\n",
            "min return: 0.9607; max return: 1.0000\n",
            "\n",
            "episode 100 in 2.39 sec\n",
            "mean reward: 0.9886\n",
            "return standard deviation: 0.0110\n",
            "min return: 0.9352; max return: 1.0000\n",
            "\n",
            "episode 101 in 2.39 sec\n",
            "mean reward: 0.9880\n",
            "return standard deviation: 0.0097\n",
            "min return: 0.9514; max return: 0.9996\n",
            "\n",
            "episode 102 in 2.38 sec\n",
            "mean reward: 0.9877\n",
            "return standard deviation: 0.0227\n",
            "min return: 0.8239; max return: 1.0000\n",
            "\n",
            "episode 103 in 2.39 sec\n",
            "mean reward: 0.9870\n",
            "return standard deviation: 0.0192\n",
            "min return: 0.8640; max return: 0.9999\n",
            "\n",
            "episode 104 in 2.41 sec\n",
            "mean reward: 0.9897\n",
            "return standard deviation: 0.0123\n",
            "min return: 0.9340; max return: 0.9997\n",
            "\n",
            "episode 105 in 2.40 sec\n",
            "mean reward: 0.9842\n",
            "return standard deviation: 0.0238\n",
            "min return: 0.8262; max return: 0.9998\n",
            "\n",
            "episode 106 in 2.43 sec\n",
            "mean reward: 0.9900\n",
            "return standard deviation: 0.0084\n",
            "min return: 0.9616; max return: 0.9999\n",
            "\n",
            "episode 107 in 2.43 sec\n",
            "mean reward: 0.9907\n",
            "return standard deviation: 0.0068\n",
            "min return: 0.9734; max return: 0.9995\n",
            "\n",
            "episode 108 in 2.42 sec\n",
            "mean reward: 0.9903\n",
            "return standard deviation: 0.0105\n",
            "min return: 0.9542; max return: 1.0000\n",
            "\n",
            "episode 109 in 2.39 sec\n",
            "mean reward: 0.9922\n",
            "return standard deviation: 0.0072\n",
            "min return: 0.9649; max return: 0.9999\n",
            "\n",
            "episode 110 in 2.44 sec\n",
            "mean reward: 0.9914\n",
            "return standard deviation: 0.0099\n",
            "min return: 0.9385; max return: 1.0000\n",
            "\n",
            "episode 111 in 2.43 sec\n",
            "mean reward: 0.9942\n",
            "return standard deviation: 0.0063\n",
            "min return: 0.9749; max return: 1.0000\n",
            "\n",
            "episode 112 in 2.37 sec\n",
            "mean reward: 0.9949\n",
            "return standard deviation: 0.0061\n",
            "min return: 0.9749; max return: 1.0000\n",
            "\n",
            "episode 113 in 2.43 sec\n",
            "mean reward: 0.9921\n",
            "return standard deviation: 0.0094\n",
            "min return: 0.9425; max return: 1.0000\n",
            "\n",
            "episode 114 in 2.42 sec\n",
            "mean reward: 0.9927\n",
            "return standard deviation: 0.0084\n",
            "min return: 0.9574; max return: 0.9999\n",
            "\n",
            "episode 115 in 2.44 sec\n",
            "mean reward: 0.9942\n",
            "return standard deviation: 0.0061\n",
            "min return: 0.9744; max return: 1.0000\n",
            "\n",
            "episode 116 in 2.39 sec\n",
            "mean reward: 0.9915\n",
            "return standard deviation: 0.0092\n",
            "min return: 0.9516; max return: 0.9999\n",
            "\n",
            "episode 117 in 2.65 sec\n",
            "mean reward: 0.9950\n",
            "return standard deviation: 0.0050\n",
            "min return: 0.9784; max return: 1.0000\n",
            "\n",
            "episode 118 in 3.02 sec\n",
            "mean reward: 0.9936\n",
            "return standard deviation: 0.0075\n",
            "min return: 0.9612; max return: 1.0000\n",
            "\n",
            "episode 119 in 2.48 sec\n",
            "mean reward: 0.9933\n",
            "return standard deviation: 0.0079\n",
            "min return: 0.9588; max return: 0.9999\n",
            "\n",
            "episode 120 in 2.40 sec\n",
            "mean reward: 0.9930\n",
            "return standard deviation: 0.0061\n",
            "min return: 0.9717; max return: 0.9998\n",
            "\n",
            "episode 121 in 2.39 sec\n",
            "mean reward: 0.9931\n",
            "return standard deviation: 0.0087\n",
            "min return: 0.9514; max return: 0.9999\n",
            "\n",
            "episode 122 in 2.42 sec\n",
            "mean reward: 0.9955\n",
            "return standard deviation: 0.0043\n",
            "min return: 0.9823; max return: 1.0000\n",
            "\n",
            "episode 123 in 2.39 sec\n",
            "mean reward: 0.9933\n",
            "return standard deviation: 0.0095\n",
            "min return: 0.9495; max return: 1.0000\n",
            "\n",
            "episode 124 in 2.43 sec\n",
            "mean reward: 0.9939\n",
            "return standard deviation: 0.0060\n",
            "min return: 0.9736; max return: 1.0000\n",
            "\n",
            "episode 125 in 2.41 sec\n",
            "mean reward: 0.9939\n",
            "return standard deviation: 0.0062\n",
            "min return: 0.9739; max return: 1.0000\n",
            "\n",
            "episode 126 in 2.39 sec\n",
            "mean reward: 0.9937\n",
            "return standard deviation: 0.0064\n",
            "min return: 0.9726; max return: 1.0000\n",
            "\n",
            "episode 127 in 2.40 sec\n",
            "mean reward: 0.9944\n",
            "return standard deviation: 0.0068\n",
            "min return: 0.9628; max return: 0.9999\n",
            "\n",
            "episode 128 in 2.41 sec\n",
            "mean reward: 0.9944\n",
            "return standard deviation: 0.0055\n",
            "min return: 0.9795; max return: 1.0000\n",
            "\n",
            "episode 129 in 2.41 sec\n",
            "mean reward: 0.9942\n",
            "return standard deviation: 0.0055\n",
            "min return: 0.9799; max return: 1.0000\n",
            "\n",
            "episode 130 in 2.42 sec\n",
            "mean reward: 0.9933\n",
            "return standard deviation: 0.0063\n",
            "min return: 0.9759; max return: 1.0000\n",
            "\n",
            "episode 131 in 2.41 sec\n",
            "mean reward: 0.9942\n",
            "return standard deviation: 0.0055\n",
            "min return: 0.9762; max return: 1.0000\n",
            "\n",
            "episode 132 in 2.47 sec\n",
            "mean reward: 0.9946\n",
            "return standard deviation: 0.0055\n",
            "min return: 0.9758; max return: 0.9999\n",
            "\n",
            "episode 133 in 2.42 sec\n",
            "mean reward: 0.9934\n",
            "return standard deviation: 0.0071\n",
            "min return: 0.9673; max return: 0.9997\n",
            "\n",
            "episode 134 in 2.41 sec\n",
            "mean reward: 0.9946\n",
            "return standard deviation: 0.0059\n",
            "min return: 0.9774; max return: 1.0000\n",
            "\n",
            "episode 135 in 2.44 sec\n",
            "mean reward: 0.9940\n",
            "return standard deviation: 0.0066\n",
            "min return: 0.9700; max return: 1.0000\n",
            "\n",
            "episode 136 in 2.44 sec\n",
            "mean reward: 0.9949\n",
            "return standard deviation: 0.0050\n",
            "min return: 0.9783; max return: 0.9999\n",
            "\n",
            "episode 137 in 2.42 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0052\n",
            "min return: 0.9738; max return: 1.0000\n",
            "\n",
            "episode 138 in 2.43 sec\n",
            "mean reward: 0.9958\n",
            "return standard deviation: 0.0043\n",
            "min return: 0.9843; max return: 1.0000\n",
            "\n",
            "episode 139 in 2.41 sec\n",
            "mean reward: 0.9943\n",
            "return standard deviation: 0.0055\n",
            "min return: 0.9804; max return: 0.9998\n",
            "\n",
            "episode 140 in 2.43 sec\n",
            "mean reward: 0.9959\n",
            "return standard deviation: 0.0044\n",
            "min return: 0.9822; max return: 1.0000\n",
            "\n",
            "episode 141 in 2.45 sec\n",
            "mean reward: 0.9939\n",
            "return standard deviation: 0.0085\n",
            "min return: 0.9514; max return: 0.9999\n",
            "\n",
            "episode 142 in 2.43 sec\n",
            "mean reward: 0.9953\n",
            "return standard deviation: 0.0044\n",
            "min return: 0.9829; max return: 1.0000\n",
            "\n",
            "episode 143 in 2.44 sec\n",
            "mean reward: 0.9950\n",
            "return standard deviation: 0.0051\n",
            "min return: 0.9755; max return: 1.0000\n",
            "\n",
            "episode 144 in 2.44 sec\n",
            "mean reward: 0.9954\n",
            "return standard deviation: 0.0057\n",
            "min return: 0.9635; max return: 1.0000\n",
            "\n",
            "episode 145 in 2.42 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0050\n",
            "min return: 0.9767; max return: 1.0000\n",
            "\n",
            "episode 146 in 2.44 sec\n",
            "mean reward: 0.9965\n",
            "return standard deviation: 0.0031\n",
            "min return: 0.9867; max return: 1.0000\n",
            "\n",
            "episode 147 in 2.46 sec\n",
            "mean reward: 0.9935\n",
            "return standard deviation: 0.0069\n",
            "min return: 0.9556; max return: 1.0000\n",
            "\n",
            "episode 148 in 2.45 sec\n",
            "mean reward: 0.9954\n",
            "return standard deviation: 0.0042\n",
            "min return: 0.9832; max return: 0.9999\n",
            "\n",
            "episode 149 in 2.43 sec\n",
            "mean reward: 0.9955\n",
            "return standard deviation: 0.0038\n",
            "min return: 0.9843; max return: 1.0000\n",
            "\n",
            "episode 150 in 2.43 sec\n",
            "mean reward: 0.9944\n",
            "return standard deviation: 0.0056\n",
            "min return: 0.9762; max return: 1.0000\n",
            "\n",
            "episode 151 in 2.43 sec\n",
            "mean reward: 0.9955\n",
            "return standard deviation: 0.0049\n",
            "min return: 0.9790; max return: 0.9998\n",
            "\n",
            "episode 152 in 2.41 sec\n",
            "mean reward: 0.9960\n",
            "return standard deviation: 0.0034\n",
            "min return: 0.9822; max return: 1.0000\n",
            "\n",
            "episode 153 in 2.42 sec\n",
            "mean reward: 0.9964\n",
            "return standard deviation: 0.0041\n",
            "min return: 0.9768; max return: 0.9999\n",
            "\n",
            "episode 154 in 2.41 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0054\n",
            "min return: 0.9726; max return: 1.0000\n",
            "\n",
            "episode 155 in 2.39 sec\n",
            "mean reward: 0.9965\n",
            "return standard deviation: 0.0030\n",
            "min return: 0.9877; max return: 1.0000\n",
            "\n",
            "episode 156 in 2.42 sec\n",
            "mean reward: 0.9955\n",
            "return standard deviation: 0.0042\n",
            "min return: 0.9793; max return: 1.0000\n",
            "\n",
            "episode 157 in 2.42 sec\n",
            "mean reward: 0.9947\n",
            "return standard deviation: 0.0051\n",
            "min return: 0.9752; max return: 0.9998\n",
            "\n",
            "episode 158 in 2.43 sec\n",
            "mean reward: 0.9950\n",
            "return standard deviation: 0.0045\n",
            "min return: 0.9802; max return: 0.9999\n",
            "\n",
            "episode 159 in 2.42 sec\n",
            "mean reward: 0.9939\n",
            "return standard deviation: 0.0077\n",
            "min return: 0.9456; max return: 1.0000\n",
            "\n",
            "episode 160 in 2.40 sec\n",
            "mean reward: 0.9948\n",
            "return standard deviation: 0.0059\n",
            "min return: 0.9693; max return: 1.0000\n",
            "\n",
            "episode 161 in 2.41 sec\n",
            "mean reward: 0.9957\n",
            "return standard deviation: 0.0048\n",
            "min return: 0.9799; max return: 1.0000\n",
            "\n",
            "episode 162 in 2.44 sec\n",
            "mean reward: 0.9941\n",
            "return standard deviation: 0.0050\n",
            "min return: 0.9791; max return: 1.0000\n",
            "\n",
            "episode 163 in 2.40 sec\n",
            "mean reward: 0.9947\n",
            "return standard deviation: 0.0052\n",
            "min return: 0.9826; max return: 1.0000\n",
            "\n",
            "episode 164 in 2.40 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0053\n",
            "min return: 0.9727; max return: 0.9999\n",
            "\n",
            "episode 165 in 2.41 sec\n",
            "mean reward: 0.9953\n",
            "return standard deviation: 0.0043\n",
            "min return: 0.9830; max return: 0.9998\n",
            "\n",
            "episode 166 in 2.41 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0044\n",
            "min return: 0.9821; max return: 1.0000\n",
            "\n",
            "episode 167 in 2.37 sec\n",
            "mean reward: 0.9953\n",
            "return standard deviation: 0.0039\n",
            "min return: 0.9851; max return: 1.0000\n",
            "\n",
            "episode 168 in 2.42 sec\n",
            "mean reward: 0.9954\n",
            "return standard deviation: 0.0038\n",
            "min return: 0.9838; max return: 1.0000\n",
            "\n",
            "episode 169 in 2.40 sec\n",
            "mean reward: 0.9953\n",
            "return standard deviation: 0.0037\n",
            "min return: 0.9820; max return: 1.0000\n",
            "\n",
            "episode 170 in 2.38 sec\n",
            "mean reward: 0.9958\n",
            "return standard deviation: 0.0043\n",
            "min return: 0.9832; max return: 0.9999\n",
            "\n",
            "episode 171 in 2.42 sec\n",
            "mean reward: 0.9957\n",
            "return standard deviation: 0.0039\n",
            "min return: 0.9852; max return: 0.9999\n",
            "\n",
            "episode 172 in 2.40 sec\n",
            "mean reward: 0.9946\n",
            "return standard deviation: 0.0045\n",
            "min return: 0.9841; max return: 1.0000\n",
            "\n",
            "episode 173 in 2.41 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0042\n",
            "min return: 0.9821; max return: 0.9999\n",
            "\n",
            "episode 174 in 2.41 sec\n",
            "mean reward: 0.9953\n",
            "return standard deviation: 0.0040\n",
            "min return: 0.9827; max return: 0.9999\n",
            "\n",
            "episode 175 in 2.39 sec\n",
            "mean reward: 0.9946\n",
            "return standard deviation: 0.0050\n",
            "min return: 0.9803; max return: 0.9999\n",
            "\n",
            "episode 176 in 2.39 sec\n",
            "mean reward: 0.9960\n",
            "return standard deviation: 0.0047\n",
            "min return: 0.9739; max return: 1.0000\n",
            "\n",
            "episode 177 in 2.38 sec\n",
            "mean reward: 0.9959\n",
            "return standard deviation: 0.0044\n",
            "min return: 0.9735; max return: 0.9999\n",
            "\n",
            "episode 178 in 2.43 sec\n",
            "mean reward: 0.9955\n",
            "return standard deviation: 0.0040\n",
            "min return: 0.9810; max return: 1.0000\n",
            "\n",
            "episode 179 in 4.59 sec\n",
            "mean reward: 0.9950\n",
            "return standard deviation: 0.0039\n",
            "min return: 0.9847; max return: 1.0000\n",
            "\n",
            "episode 180 in 4.40 sec\n",
            "mean reward: 0.9948\n",
            "return standard deviation: 0.0045\n",
            "min return: 0.9773; max return: 1.0000\n",
            "\n",
            "episode 181 in 2.39 sec\n",
            "mean reward: 0.9950\n",
            "return standard deviation: 0.0042\n",
            "min return: 0.9807; max return: 0.9999\n",
            "\n",
            "episode 182 in 2.40 sec\n",
            "mean reward: 0.9957\n",
            "return standard deviation: 0.0054\n",
            "min return: 0.9655; max return: 0.9999\n",
            "\n",
            "episode 183 in 2.37 sec\n",
            "mean reward: 0.9951\n",
            "return standard deviation: 0.0041\n",
            "min return: 0.9780; max return: 0.9998\n",
            "\n",
            "episode 184 in 2.40 sec\n",
            "mean reward: 0.9949\n",
            "return standard deviation: 0.0039\n",
            "min return: 0.9840; max return: 1.0000\n",
            "\n",
            "episode 185 in 2.41 sec\n",
            "mean reward: 0.9949\n",
            "return standard deviation: 0.0043\n",
            "min return: 0.9822; max return: 1.0000\n",
            "\n",
            "episode 186 in 2.45 sec\n",
            "mean reward: 0.9951\n",
            "return standard deviation: 0.0036\n",
            "min return: 0.9851; max return: 1.0000\n",
            "\n",
            "episode 187 in 2.35 sec\n",
            "mean reward: 0.9953\n",
            "return standard deviation: 0.0043\n",
            "min return: 0.9812; max return: 1.0000\n",
            "\n",
            "episode 188 in 2.41 sec\n",
            "mean reward: 0.9962\n",
            "return standard deviation: 0.0034\n",
            "min return: 0.9847; max return: 1.0000\n",
            "\n",
            "episode 189 in 2.48 sec\n",
            "mean reward: 0.9957\n",
            "return standard deviation: 0.0038\n",
            "min return: 0.9820; max return: 0.9999\n",
            "\n",
            "episode 190 in 2.99 sec\n",
            "mean reward: 0.9959\n",
            "return standard deviation: 0.0038\n",
            "min return: 0.9800; max return: 1.0000\n",
            "\n",
            "episode 191 in 2.76 sec\n",
            "mean reward: 0.9953\n",
            "return standard deviation: 0.0045\n",
            "min return: 0.9712; max return: 0.9999\n",
            "\n",
            "episode 192 in 2.45 sec\n",
            "mean reward: 0.9956\n",
            "return standard deviation: 0.0037\n",
            "min return: 0.9837; max return: 0.9999\n",
            "\n",
            "episode 193 in 2.43 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0044\n",
            "min return: 0.9777; max return: 1.0000\n",
            "\n",
            "episode 194 in 2.45 sec\n",
            "mean reward: 0.9952\n",
            "return standard deviation: 0.0049\n",
            "min return: 0.9713; max return: 1.0000\n",
            "\n",
            "episode 195 in 2.42 sec\n",
            "mean reward: 0.9957\n",
            "return standard deviation: 0.0035\n",
            "min return: 0.9862; max return: 0.9999\n",
            "\n",
            "episode 196 in 2.42 sec\n",
            "mean reward: 0.9956\n",
            "return standard deviation: 0.0035\n",
            "min return: 0.9849; max return: 0.9999\n",
            "\n",
            "episode 197 in 2.44 sec\n",
            "mean reward: 0.9957\n",
            "return standard deviation: 0.0042\n",
            "min return: 0.9844; max return: 1.0000\n",
            "\n",
            "episode 198 in 2.42 sec\n",
            "mean reward: 0.9959\n",
            "return standard deviation: 0.0045\n",
            "min return: 0.9738; max return: 1.0000\n",
            "\n",
            "episode 199 in 2.42 sec\n",
            "mean reward: 0.9942\n",
            "return standard deviation: 0.0053\n",
            "min return: 0.9787; max return: 1.0000\n",
            "\n",
            "episode 200 in 2.39 sec\n",
            "mean reward: 0.9946\n",
            "return standard deviation: 0.0053\n",
            "min return: 0.9806; max return: 0.9999\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Train model\n",
        "\n",
        "import time\n",
        "\n",
        "# define number of training episodes\n",
        "N_episodes = 201 # total number of training episodes\n",
        "N_MC = 64 #128 # number of trajectories in the batch\n",
        "\n",
        "\n",
        "# preallocate data using arrays initialized with zeros\n",
        "state=np.zeros((2,), dtype=np.float32)\n",
        "    \n",
        "states = np.zeros((N_MC, env.n_time_steps,2), dtype=np.float32)\n",
        "actions = np.zeros((N_MC, env.n_time_steps), dtype=np.int64)\n",
        "returns = np.zeros((N_MC, env.n_time_steps), dtype=np.float32)\n",
        "    \n",
        "# mean reward at the end of the episode\n",
        "mean_final_reward = np.zeros(N_episodes, dtype=np.float32)\n",
        "# standard deviation of the reward at the end of the episode\n",
        "std_final_reward = np.zeros_like(mean_final_reward)\n",
        "# batch minimum at the end of the episode\n",
        "min_final_reward = np.zeros_like(mean_final_reward)\n",
        "# batch maximum at the end of the episode\n",
        "max_final_reward = np.zeros_like(mean_final_reward)\n",
        "\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "\n",
        "# set the initial model parameters in the optimizer\n",
        "opt_state = opt_init(inital_params)\n",
        "\n",
        "# loop over the number of training episodes\n",
        "for episode in range(N_episodes): \n",
        "    \n",
        "    ### record time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # get current policy  network params\n",
        "    current_params = get_params(opt_state)\n",
        "    \n",
        "    # MC sample\n",
        "    for j in range(N_MC):\n",
        "        \n",
        "        # reset environment to a random initial state\n",
        "        #env.reset(random=False) # fixed initial state\n",
        "        env.reset(random=True) # Haar-random initial state (i.e. uniformly sampled on the sphere)\n",
        "    \n",
        "        # zero rewards array (auxiliary array to store the rewards, and help compute the returns)\n",
        "        rewards = np.zeros((env.n_time_steps, ), dtype=np.float32)\n",
        "    \n",
        "        # loop over steps in an episode\n",
        "        for time_step in range(env.n_time_steps):\n",
        "\n",
        "            # select state\n",
        "            state[:] = env.state[:]\n",
        "            states[j,time_step,:] = state\n",
        "\n",
        "            # select an action according to current policy\n",
        "            pi_s = np.exp( predict(current_params, state) )\n",
        "            action = np.random.choice(env.actions, p = pi_s)\n",
        "            actions[j,time_step] = action\n",
        "\n",
        "            # take an environment step\n",
        "            state[:], reward, _ = env.step(action)\n",
        "\n",
        "            # store reward\n",
        "            rewards[time_step] = reward\n",
        "            \n",
        "            \n",
        "        # compute reward-to-go \n",
        "        returns[j,:] = jnp.cumsum(rewards[::-1])[::-1]\n",
        "        \n",
        "        \n",
        "            \n",
        "    # define batch of data\n",
        "    trajectory_batch = (states, actions, returns)\n",
        "    \n",
        "    # update model\n",
        "    opt_state = update(episode, opt_state, trajectory_batch)\n",
        "            \n",
        "    ### record time needed for a single epoch\n",
        "    episode_time = time.time() - start_time\n",
        "    \n",
        "    # check performance\n",
        "    mean_final_reward[episode]=jnp.mean(returns[:,-1])\n",
        "    std_final_reward[episode] =jnp.std(returns[:,-1])\n",
        "    min_final_reward[episode], max_final_reward[episode] = np.min(returns[:,-1]), np.max(returns[:,-1])\n",
        "\n",
        "    \n",
        "    # print results every 10 epochs\n",
        "    #if episode % 5 == 0:\n",
        "    print(\"episode {} in {:0.2f} sec\".format(episode, episode_time))\n",
        "    print(\"mean reward: {:0.4f}\".format(mean_final_reward[episode]) )\n",
        "    print(\"return standard deviation: {:0.4f}\".format(std_final_reward[episode]) )\n",
        "    print(\"min return: {:0.4f}; max return: {:0.4f}\\n\".format(min_final_reward[episode], max_final_reward[episode]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kww9Njff2tlV"
      },
      "source": [
        "## Plot the training curves\n",
        "\n",
        "Plot the mean final reward at each episode, and its variance. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VVL-xZlg2tlV",
        "outputId": "1570c4d0-d309-4d00-879b-473b77edef36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "# static plots\n",
        "%matplotlib inline \n",
        "\n",
        "### plot and examine learning curves\n",
        "\n",
        "episodes=list(range(N_episodes))\n",
        "\n",
        "plt.plot(episodes, mean_final_reward, '-k', label='mean final reward' )\n",
        "plt.fill_between(episodes, \n",
        "                 mean_final_reward-0.5*std_final_reward, \n",
        "                 mean_final_reward+0.5*std_final_reward, \n",
        "                 color='k', \n",
        "                 alpha=0.25)\n",
        "\n",
        "plt.plot(episodes, min_final_reward, '--b' , label='min final reward' )\n",
        "plt.plot(episodes, max_final_reward, '--r' , label='max final reward' )\n",
        "\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('final reward')\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx0eLXIk2tlW"
      },
      "source": [
        "## Questions\n",
        "\n",
        "0. Try out different batch sizes and hyperparameters (including different network architectures). Can you improve the performance?\n",
        "\n",
        "1. Explore the final batch of trajectories. Check the sequence of actions. Can you make sense of the solution found by the agent? Hint: think of the dynamics on the Bloch sphere and try to visualize the trajectory there.\n",
        "\n",
        "2. Compare the Policy Gradient method to conventional optimal control: can optimal control give you a control protocol that works for all states? Why or why not? \n",
        "\n",
        "3. Take one of the high-reward tranjectories in the final batch of data. Now perturb it manually at a few time steps in the first half of the protocol such that it no longer produces an optimal reward (you would have to add a function to the environment which evalues a given trajectory). Last, use the policy to see how it would react to those perturbations in real time. Will it correct on-line for the introduced mistakes (i.e. before the opisode is over)? \n",
        "\n",
        "4. Find ways to visualize the policy. What is a meaningful way to do that? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYUkJoq62tlW"
      },
      "source": [
        "## Advanced Problems\n",
        "\n",
        "1. What is the initial state distribution $p(s_0)$ in the implementation above? Check the performance of the PG algorith if $p(s_0)$ is \n",
        "    * a delta distribution \n",
        "    * a compactly-supported uniform distribution over some sector of the sphere (say a cap around the south pole)\n",
        "    * a Gaussian distribution with non-compact support \n",
        "    \n",
        "2. Introduce small Gaussian noise to the rewards, e.g. $r(s,a) \\to r(s,a) + \\delta r$ where $\\delta r \\sim \\mathcal{N}(0,\\delta)$ for some noise strength $\\delta$. Does this lead to a serious performance drop as you vary $\\delta\\in[0,0.5]$? Why or why not?\n",
        "    \n",
        "2. The loop over the $N_{MC}$ trajectories slows down the algorithm significantly. Consider ways to speed up the evaluation of a single PG iteration. This may include a modification of the environment `QubitEnv` or the use of parallelization software (see JAX's function [`vmap`](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap) and [`pmap`](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)). \n",
        "    \n",
        "3. Change the environment `QubitEnv` to define a nonepisodic task. Additionally, introduce a \"stop\" action so that when the agent bring the RL state close to $s_\\mathrm{target}$ the episode comes to an end and the environment is reset. This would require you to also modify the Policy Gradient implementation above because episodes now can have different length.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onGMmTmtvkHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RL_course",
      "language": "python",
      "name": "rl_course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "latex_metadata": {
      "affiliation": "Faculty of Physics, Sofia University, 5 James Bourchier Blvd., 1164 Sofia, Bulgaria",
      "author": "Marin Bukov",
      "title": "Reinforcement Learning Course: WiSe 2020/21"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}